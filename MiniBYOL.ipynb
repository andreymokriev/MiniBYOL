{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epochs = 45\n",
    "tau = 0.98  # EMA коэффициент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аугментация изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(28, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_plain = transforms.ToTensor()  # Для линейной оценки\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform_plain)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2, 1),  # 28x28 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),  # 14x14 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, feature_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проекционная и предсказательная голова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim=128, hidden_dim=256, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, in_dim=128, hidden_dim=256, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(nn.Module):\n",
    "    def __init__(self, encoder, tau=0.99):\n",
    "        super().__init__()\n",
    "        self.online_encoder = encoder\n",
    "        self.target_encoder = deepcopy(encoder)\n",
    "        self.projector_online = ProjectionHead()\n",
    "        self.projector_target = deepcopy(self.projector_online)\n",
    "        self.predictor = PredictionHead()\n",
    "        self.tau = tau\n",
    "        \n",
    "        for p in self.target_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.projector_target.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _update_target(self):\n",
    "        for param_q, param_k in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\n",
    "            param_k.data = self.tau * param_k.data + (1 - self.tau) * param_q.data\n",
    "        for param_q, param_k in zip(self.projector_online.parameters(), self.projector_target.parameters()):\n",
    "            param_k.data = self.tau * param_k.data + (1 - self.tau) * param_q.data\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # online\n",
    "        q1 = self.predictor(self.projector_online(self.online_encoder(x1)))\n",
    "        q2 = self.predictor(self.projector_online(self.online_encoder(x2)))\n",
    "        # target\n",
    "        with torch.no_grad():\n",
    "            z1 = self.projector_target(self.target_encoder(x1))\n",
    "            z2 = self.projector_target(self.target_encoder(x2))\n",
    "        return q1, q2, z1.detach(), z2.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(q, z):\n",
    "    q = F.normalize(q, dim=-1)\n",
    "    z = F.normalize(z, dim=-1)\n",
    "    return 2 - 2 * (q * z).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "byol_losses = []\n",
    "linear_losses = []\n",
    "linear_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, BYOL Loss = 0.7912\n",
      "Эпоха 2, BYOL Loss = 0.6209\n",
      "Эпоха 3, BYOL Loss = 0.8714\n",
      "Эпоха 4, BYOL Loss = 0.6629\n",
      "Эпоха 5, BYOL Loss = 0.4309\n",
      "Эпоха 6, BYOL Loss = 0.3961\n",
      "Эпоха 7, BYOL Loss = 0.4984\n",
      "Эпоха 8, BYOL Loss = 0.8659\n",
      "Эпоха 9, BYOL Loss = 0.8070\n",
      "Эпоха 10, BYOL Loss = 0.7648\n",
      "Эпоха 11, BYOL Loss = 0.8293\n",
      "Эпоха 12, BYOL Loss = 0.9720\n",
      "Эпоха 13, BYOL Loss = 0.9365\n",
      "Эпоха 14, BYOL Loss = 1.0937\n",
      "Эпоха 15, BYOL Loss = 1.3475\n",
      "Эпоха 16, BYOL Loss = 1.0642\n",
      "Эпоха 17, BYOL Loss = 0.7580\n",
      "Эпоха 18, BYOL Loss = 0.5762\n",
      "Эпоха 19, BYOL Loss = 0.4906\n",
      "Эпоха 20, BYOL Loss = 0.4757\n",
      "Эпоха 21, BYOL Loss = 0.4628\n",
      "Эпоха 22, BYOL Loss = 0.4283\n",
      "Эпоха 23, BYOL Loss = 0.4222\n",
      "Эпоха 24, BYOL Loss = 0.3697\n",
      "Эпоха 25, BYOL Loss = 0.3480\n",
      "Эпоха 26, BYOL Loss = 0.4619\n",
      "Эпоха 27, BYOL Loss = 0.4077\n",
      "Эпоха 28, BYOL Loss = 0.4415\n",
      "Эпоха 29, BYOL Loss = 0.5137\n",
      "Эпоха 30, BYOL Loss = 0.5171\n",
      "Эпоха 31, BYOL Loss = 0.6851\n",
      "Эпоха 32, BYOL Loss = 0.6804\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder().to(device)\n",
    "model = BYOL(encoder, tau).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (x1, _), (x2, _) in zip(train_loader, train_loader):\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "\n",
    "        q1, q2, z1, z2 = model(x1, x2)\n",
    "        loss = loss_fn(q1, z2) / 2 + loss_fn(q2, z1) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model._update_target()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    byol_losses.append(avg_loss)\n",
    "    print(f\"Эпоха {epoch}, BYOL Loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заморозка эмбеддингов и линейная голова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(x)\n",
    "        return self.fc(features)\n",
    "\n",
    "linear_model = LinearClassifier(model.online_encoder).to(device)\n",
    "optimizer = optim.Adam(linear_model.fc.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    linear_model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = linear_model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (output.argmax(1) == y).sum().item()\n",
    "\n",
    "    acc = 100. * correct / len(train_loader.dataset)\n",
    "    linear_losses.append(total_loss / len(train_loader))\n",
    "    linear_accs.append(acc)\n",
    "    print(f\"[Linear Eval] Эпоха {epoch}, Loss={total_loss/len(train_loader):.4f}, Точность={acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# График BYOL loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(byol_losses, marker='o')\n",
    "plt.title(\"Сходимость BYOL Loss\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"BYOL Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Графики линейной головы\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(linear_losses, marker='o', color='orange')\n",
    "plt.title(\"Сходимость Loss линейной головы\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(linear_accs, marker='o', color='green')\n",
    "plt.title(\"Точность линейной головы\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE Распределение эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Собираем эмбеддинги для тестового датасета\n",
    "# -----------------------------\n",
    "model.online_encoder.eval()\n",
    "features = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        feats = model.online_encoder(x)  # эмбеддинги размерности 128\n",
    "        features.append(feats.cpu())\n",
    "        labels_list.append(y)\n",
    "\n",
    "features = torch.cat(features).numpy()  # (N, 128)\n",
    "labels = torch.cat(labels_list).numpy()  # (N,)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. t-SNE\n",
    "# -----------------------------\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "features_2d = tsne.fit_transform(features)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Визуализация\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap='tab10', s=5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Digits\")\n",
    "plt.title(\"t-SNE визуализация эмбеддингов BYOL на MNIST\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50e theta 0.99 -  0.4760\n",
    "50e theta 0.98 -  0.1897"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
